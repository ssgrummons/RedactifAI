# ============================================================================
# RedactifAI Configuration
# ============================================================================
# Copy this file to .env and fill in your values
# Docker Compose will automatically load .env if it exists


# ============================================================================
# DATABASE CONFIGURATION
# ============================================================================
# PostgreSQL connection details
# These are used by both the API and Celery workers to connect to the database

DB_USER=redactifai
DB_PASSWORD=redactifai_password
DB_HOST=postgres
DB_PORT=5432
DB_NAME=redactifai

# Note: DB_HOST should be 'postgres' when running in Docker Compose
# For local development outside Docker, use 'localhost'


# ============================================================================
# CELERY & REDIS CONFIGURATION
# ============================================================================
# Redis is used as the message broker and result backend for Celery tasks

CELERY_BROKER_URL=redis://redis:6379/0
CELERY_RESULT_BACKEND=redis://redis:6379/1

# Task retry configuration
CELERY_TASK_MAX_RETRIES=3
CELERY_TASK_TIME_LIMIT=3600              # Max 1 hour per task
CELERY_TASK_SOFT_TIME_LIMIT=3300         # Soft limit at 55 minutes
CELERY_TASK_DEFAULT_RETRY_DELAY=60       # Wait 60 seconds between retries

# For testing: set to 'true' to run tasks synchronously (no async queue)
CELERY_TASK_ALWAYS_EAGER=false


# ============================================================================
# STORAGE CONFIGURATION
# ============================================================================
# RedactifAI uses two separate buckets for security:
# - PHI bucket: Stores uploaded documents (deleted after processing)
# - Clean bucket: Stores de-identified outputs (retained)

# Storage backend: 's3' for MinIO/AWS S3, 'local' for filesystem
STORAGE_BACKEND=s3

# S3/MinIO Configuration
# ------------------------------------------------------------------------------
# For local development with MinIO (included in docker-compose):
STORAGE_S3_ENDPOINT_URL=http://minio:9000
STORAGE_S3_ACCESS_KEY=minioadmin
STORAGE_S3_SECRET_KEY=minioadmin123
STORAGE_S3_REGION=us-east-1

# For production AWS S3, comment out STORAGE_S3_ENDPOINT_URL and set:
# STORAGE_S3_ENDPOINT_URL=          # Leave blank for real AWS S3
# STORAGE_S3_ACCESS_KEY=AKIA...     # Your AWS access key
# STORAGE_S3_SECRET_KEY=...         # Your AWS secret key
# STORAGE_S3_REGION=us-east-1       # Your AWS region

# Bucket names (must match what's created in docker-compose or AWS)
STORAGE_PHI_BUCKET=redactifai-phi-uploads
STORAGE_CLEAN_BUCKET=redactifai-clean-outputs

# Bucket prefixes (optional path prefixes within buckets)
STORAGE_PHI_PREFIX=uploads/
STORAGE_CLEAN_PREFIX=masked/

# Local Storage Configuration (if STORAGE_BACKEND=local)
# ------------------------------------------------------------------------------
# Only used when STORAGE_BACKEND=local
# STORAGE_LOCAL_PHI_PATH=/tmp/redactifai/phi
# STORAGE_LOCAL_CLEAN_PATH=/tmp/redactifai/clean


# ============================================================================
# OCR & PHI DETECTION PROVIDER
# ============================================================================
# Provider selection: 'azure', 'aws', or 'mock'
# - 'mock': Uses regex-based detection (for development/testing only)
# - 'azure': Azure Cognitive Services (Document Intelligence + Language)
# - 'aws': AWS Textract + Comprehend Medical

DEFAULT_PROVIDER=mock

# Default masking level for all jobs (unless overridden in API request)
# Options: 'safe_harbor', 'limited_dataset', 'custom'
DEFAULT_MASKING_LEVEL=safe_harbor


# Azure Configuration (only needed if DEFAULT_PROVIDER=azure)
# ------------------------------------------------------------------------------
# Get these from Azure Portal > Cognitive Services

# Document Intelligence (OCR)
AZURE_DOCUMENT_INTELLIGENCE_ENDPOINT=https://your-resource.cognitiveservices.azure.com/
AZURE_DOCUMENT_INTELLIGENCE_KEY=your_key_here

# Language Service (PHI Detection)
AZURE_LANGUAGE_ENDPOINT=https://your-resource.cognitiveservices.azure.com/
AZURE_LANGUAGE_KEY=your_key_here


# AWS Configuration (only needed if DEFAULT_PROVIDER=aws)
# ------------------------------------------------------------------------------
# Get these from AWS IAM

AWS_ACCESS_KEY_ID=AKIA...
AWS_SECRET_ACCESS_KEY=your_secret_key_here
AWS_REGION=us-east-1

# Textract region (defaults to AWS_REGION if not set)
# AWS_TEXTRACT_REGION=us-east-1

# Comprehend Medical region (must be a region that supports Comprehend Medical)
# Supported: us-east-1, us-east-2, us-west-2, ap-southeast-2, ca-central-1, eu-west-1, eu-west-2
AWS_COMPREHEND_REGION=us-east-1


# ============================================================================
# API CONFIGURATION
# ============================================================================

# API server settings
API_HOST=0.0.0.0
API_PORT=8000
API_WORKERS=1

# File upload limits
MAX_FILE_SIZE_MB=50

# Image processing
MASKING_PADDING_PX=5                    # Padding around redaction boxes
MAX_OCR_SIZE_MB=10.0                    # Max size before compression

# Logging
LOG_LEVEL=INFO                          # DEBUG, INFO, WARN, ERROR
LOG_FORMAT=json                         # json or text


# ============================================================================
# CONFIGURATION COMBINATIONS GUIDE
# ============================================================================

# Local Development (Default)
# ------------------------------------------------------------------------------
# Uses mock services, MinIO for storage, local PostgreSQL and Redis
# DEFAULT_PROVIDER=mock
# STORAGE_BACKEND=s3
# STORAGE_S3_ENDPOINT_URL=http://minio:9000
# No cloud credentials needed

# Production with Azure
# ------------------------------------------------------------------------------
# Uses Azure for OCR/PHI, AWS S3 for storage
# DEFAULT_PROVIDER=azure
# STORAGE_BACKEND=s3
# STORAGE_S3_ENDPOINT_URL=              # Blank = use real AWS S3
# STORAGE_S3_ACCESS_KEY=AKIA...
# STORAGE_S3_SECRET_KEY=...
# AZURE_DOCUMENT_INTELLIGENCE_ENDPOINT=...
# AZURE_DOCUMENT_INTELLIGENCE_KEY=...
# AZURE_LANGUAGE_ENDPOINT=...
# AZURE_LANGUAGE_KEY=...

# Production with AWS
# ------------------------------------------------------------------------------
# Uses AWS for OCR/PHI/storage
# DEFAULT_PROVIDER=aws
# STORAGE_BACKEND=s3
# STORAGE_S3_ENDPOINT_URL=              # Blank = use real AWS S3
# STORAGE_S3_ACCESS_KEY=AKIA...
# STORAGE_S3_SECRET_KEY=...
# AWS_REGION=us-east-1
# AWS_COMPREHEND_REGION=us-east-1

# Filesystem Storage (No Cloud)
# ------------------------------------------------------------------------------
# Uses local filesystem instead of S3/MinIO
# DEFAULT_PROVIDER=mock                 # or azure/aws for OCR/PHI
# STORAGE_BACKEND=local
# STORAGE_LOCAL_PHI_PATH=/var/redactifai/phi
# STORAGE_LOCAL_CLEAN_PATH=/var/redactifai/clean


# ============================================================================
# SECURITY NOTES
# ============================================================================

# 1. Never commit .env file to git (it's in .gitignore)
# 2. For production:
#    - Use strong, unique passwords for DB_PASSWORD
#    - Rotate STORAGE_S3_SECRET_KEY regularly
#    - Store credentials in a secrets manager (AWS Secrets Manager, Azure Key Vault)
#    - Sign BAAs with cloud providers for HIPAA compliance
# 3. PHI bucket should have:
#    - Lifecycle policy to delete after 24 hours
#    - Encryption at rest
#    - Access logging enabled
#    - Strict IAM/RBAC policies
# 4. Implement proper authentication (replace NoOpAuth) before production use